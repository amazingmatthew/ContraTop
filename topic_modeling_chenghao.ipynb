{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j-z5FHvZrp7i"
      },
      "outputs": [],
      "source": [
        "pip install sentence_transformers transformers hdbscan umap-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ytq5eh78rq7H"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import hdbscan\n",
        "import umap.umap_ as umap\n",
        "import re\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sentence_transformers import SentenceTransformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2-dNaGvortGv"
      },
      "outputs": [],
      "source": [
        "class candidate():\n",
        "\n",
        "    def __init__(self, text,\n",
        "                 mode: str = 'n_gram',\n",
        "                 ngram_range = (2,4),\n",
        "                 feature_1gram: int = 2000,\n",
        "                 feature_mgram: int = 10000,\n",
        "                 punctuation: str = '[?!,.]'\n",
        "                 ):\n",
        "        self.corpus = np.array(text)\n",
        "        self.mode = mode\n",
        "\n",
        "        self.ngram_range = ngram_range\n",
        "        self.feature_1gram = feature_1gram\n",
        "        self.feature_mgram = feature_mgram\n",
        "        self.punctuation = punctuation\n",
        "\n",
        "    def lower(self):\n",
        "        text = self.corpus\n",
        "        text = text.astype('U')\n",
        "        text = np.char.lower(text)\n",
        "\n",
        "        return text\n",
        "\n",
        "    def n_gram(self):\n",
        "        ngram_range = self.ngram_range\n",
        "        feature_1gram = self.feature_1gram\n",
        "        feature_mgram = self.feature_mgram\n",
        "\n",
        "        text = self.lower()\n",
        "        vectorizer = CountVectorizer(max_features = feature_1gram)\n",
        "        vectorizer2 = CountVectorizer(analyzer='word', ngram_range=ngram_range, max_features = feature_mgram)\n",
        "        vectorizer.fit(text)\n",
        "        vectorizer2.fit(text)\n",
        "        vocab = vectorizer.get_feature_names_out()\n",
        "        phrase = vectorizer2.get_feature_names_out()\n",
        "        total = list(np.concatenate([vocab,phrase]))\n",
        "\n",
        "        return total\n",
        "\n",
        "    def sentence_level(self):\n",
        "\n",
        "        text = self.corpus\n",
        "        punctuation = self.punctuation\n",
        "\n",
        "        sentence_list = []\n",
        "        for index, doc in enumerate(text):\n",
        "          doc = doc.strip()\n",
        "          doc = re.split(punctuation,doc)\n",
        "          doc = [sent.strip() for sent in doc]\n",
        "          doc = list(filter(None, doc))\n",
        "          sentence_list.extend(doc)\n",
        "        return list(set(sentence_list))\n",
        "\n",
        "    def document_level(self):\n",
        "\n",
        "        text = list(self.corpus)\n",
        "        return list(set(text))\n",
        "\n",
        "    def build_vocab(self):\n",
        "        if self.mode == 'n_gram':\n",
        "             total = self.n_gram()\n",
        "        elif self.mode == 'sentence_level':\n",
        "             total = self.sentence_level()\n",
        "        elif self.mode == 'document_level':\n",
        "             total = self.document_level()\n",
        "\n",
        "        return total\n",
        "\n",
        "\n",
        "class pipeline():\n",
        "\n",
        "    def __init__(self, text, model = 'all-mpnet-base-v2', device = 'cuda'):\n",
        "        self.corpus = text\n",
        "        self.encoder = SentenceTransformer(model, device = device)\n",
        "\n",
        "    def encoding(self):\n",
        "        embedding = self.encoder.encode(self.corpus)\n",
        "        globals()['embedding'] = embedding\n",
        "        return embedding\n",
        "\n",
        "    def DR(self, embedding, dimension = 5): # dimensionality reduction\n",
        "\n",
        "        reducer = umap.UMAP(random_state=42,n_components=dimension)\n",
        "        embedding = reducer.fit_transform(embedding)\n",
        "        return embedding\n",
        "\n",
        "    def clustering(self, embedding, min_cluster_size = 2):\n",
        "        clusterer = hdbscan.HDBSCAN(min_cluster_size=min_cluster_size)\n",
        "        cluster_labels = clusterer.fit_predict(embedding)\n",
        "        outlier_scores = clusterer.outlier_scores_\n",
        "        return cluster_labels, outlier_scores\n",
        "\n",
        "    def agglomerative_clustering(self, embedding, n_clusters = 5):\n",
        "        clustering = AgglomerativeClustering(n_clusters=n_clusters).fit(embedding)\n",
        "        cluster_labels = clustering.labels_\n",
        "        return cluster_labels, _\n",
        "\n",
        "    def centroid(self, embedding, candidate_vocab, cluster_labels):\n",
        "\n",
        "        text = self.corpus\n",
        "        encoder = self.encoder\n",
        "        centroids = {}\n",
        "        rep = embedding\n",
        "        rep_rep = encoder.encode(candidate_vocab)\n",
        "        total = candidate_vocab\n",
        "\n",
        "        globals()['frame'] = pd.DataFrame()\n",
        "        globals()['frame']['text'] = text\n",
        "        globals()['frame']['label'] = cluster_labels\n",
        "\n",
        "        for m in list(set(cluster_labels)):\n",
        "            index = frame[frame['label'] == m].index\n",
        "            subset = rep[index]\n",
        "            centroid = np.mean(rep[index],axis = 0)\n",
        "            centroids[m] = centroid\n",
        "\n",
        "        centroid_keywords = {}\n",
        "        for key in centroids.keys():\n",
        "            centroid = centroids[key]\n",
        "            similarity = cosine_similarity([centroid],rep_rep)\n",
        "            centroid_keyword_index = similarity[0].argsort()[-3:][::-1]\n",
        "            centroid_keywords[key] = [total[i] for i in centroid_keyword_index]\n",
        "\n",
        "        return centroid_keywords\n",
        "\n",
        "\n",
        "    def pipeline(self, candidate_vocab, dimension = 5, clustering_method = 'agglomerative',\\\n",
        "                 min_cluster_size = 2, n_clusters = 5):\n",
        "\n",
        "        embedding = self.encoding()\n",
        "        reduced_embedding = self.DR(embedding, dimension = dimension)\n",
        "\n",
        "        globals()['reduced_embedding'] = reduced_embedding\n",
        "        if clustering_method == 'agglomerative':\n",
        "            cluster_labels, outlier_scores = self.agglomerative_clustering\\\n",
        "            (reduced_embedding, n_clusters = n_clusters)\n",
        "        if clustering_method == 'hdbscan':\n",
        "            cluster_labels, outlier_scores = self.clustering\\\n",
        "            (reduced_embedding, min_cluster_size = min_cluster_size)\n",
        "        centroid_keywords = self.centroid(embedding, candidate_vocab, cluster_labels)\n",
        "        return cluster_labels, outlier_scores, centroid_keywords"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
