{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j-z5FHvZrp7i"
      },
      "outputs": [],
      "source": [
        "pip install sentence_transformers transformers hdbscan umap-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ytq5eh78rq7H"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import hdbscan\n",
        "import umap.umap_ as umap\n",
        "import re\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sentence_transformers import SentenceTransformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2-dNaGvortGv"
      },
      "outputs": [],
      "source": [
        "class TopicModeling:\n",
        "    def __init__(self, text, model='all-mpnet-base-v2', device='cuda', self_sim_threshold=0.5):\n",
        "        self.corpus = text\n",
        "        self.encoder = SentenceTransformer(model, device=device)\n",
        "        self.tokenizer = self.encoder.tokenizer\n",
        "        self.self_sim_threshold = self_sim_threshold\n",
        "\n",
        "    def encoding(self):\n",
        "        embedding = self.encoder.encode(self.corpus)\n",
        "        globals()['embedding'] = embedding\n",
        "        return embedding\n",
        "\n",
        "    def DR(self, embedding, dimension=5):\n",
        "        reducer = umap.UMAP(random_state=42, n_components=dimension)\n",
        "        embedding = reducer.fit_transform(embedding)\n",
        "        return embedding\n",
        "\n",
        "    def clustering(self, embedding, min_cluster_size=2):\n",
        "        clusterer = hdbscan.HDBSCAN(min_cluster_size=min_cluster_size)\n",
        "        cluster_labels = clusterer.fit_predict(embedding)\n",
        "        outlier_scores = clusterer.outlier_scores_\n",
        "        return cluster_labels, outlier_scores\n",
        "\n",
        "    def agglomerative_clustering(self, embedding, n_clusters=5):\n",
        "        clustering = AgglomerativeClustering(n_clusters=n_clusters).fit(embedding)\n",
        "        cluster_labels = clustering.labels_\n",
        "        return cluster_labels, _\n",
        "\n",
        "    def self_similarity(self, all_hidden_states, token_list, inference_list):\n",
        "        ss_score = {}\n",
        "\n",
        "        temp = all_hidden_states[-1]\n",
        "\n",
        "        for token in tqdm(token_list, desc='Token Progress'):\n",
        "            token_embeddings = []\n",
        "\n",
        "            for (sentence_index, token_index) in inference_list[token]:\n",
        "                token_embeddings.append(np.array(temp[sentence_index][token_index]))\n",
        "\n",
        "            token_embeddings = np.array(token_embeddings)\n",
        "            sim_matrix = cosine_similarity(token_embeddings, token_embeddings)\n",
        "\n",
        "            if len(sim_matrix) != 1:\n",
        "                self_similarity = round((np.sum(sim_matrix) - len(sim_matrix)) / (len(sim_matrix) * (len(sim_matrix) - 1)), 3)\n",
        "                ss_score[token] = self_similarity\n",
        "\n",
        "        return ss_score\n",
        "\n",
        "    def build_candidates(self, corpus, encoder):\n",
        "        print(\"Tokenizing text with model:\", encoder.model_name)\n",
        "        encoded_input = self.tokenizer(corpus, padding=True, truncation=True, return_tensors='pt')\n",
        "        print(\"Finished Tokenizing:\", encoder.model_name)\n",
        "        tokenized = encoded_input['input_ids'].tolist()\n",
        "        token_list = list(itertools.chain.from_iterable(tokenized))\n",
        "        counter = Counter(token_list)\n",
        "\n",
        "        index_count = [(index, count) for (index, count) in counter.most_common() if count >= 5]\n",
        "        candidate_vocab = [index for (index, count) in counter.most_common() if count >= 5]\n",
        "\n",
        "        all_hidden_states = encoder.model.get_all_hidden_states(encoder.encode(corpus))\n",
        "\n",
        "        inference_list = {}\n",
        "        for n in set(candidate_vocab):\n",
        "            position_list = []\n",
        "            for sen_index, sen in enumerate(tokenized):\n",
        "                if n in sen:\n",
        "                    token_index = sen.index(n)\n",
        "                    position_list.append((sen_index, token_index))\n",
        "            inference_list[n] = position_list\n",
        "\n",
        "        ss_score = self.self_similarity(all_hidden_states, candidate_vocab, inference_list)\n",
        "        filtered_candidate_vocab = [token for token, self_sim in ss_score.items() if self_sim >= self.self_sim_threshold]\n",
        "\n",
        "        return filtered_candidate_vocab\n",
        "        \n",
        "    def centroid(self, embedding, candidate_vocab, cluster_labels):\n",
        "        text = self.corpus\n",
        "        encoder = self.encoder\n",
        "        centroids = {}\n",
        "        rep = embedding\n",
        "        rep_rep = encoder.encode(candidate_vocab)\n",
        "        total = candidate_vocab\n",
        "\n",
        "        globals()['frame'] = pd.DataFrame()\n",
        "        globals()['frame']['text'] = text\n",
        "        globals()['frame']['label'] = cluster_labels\n",
        "\n",
        "        for m in list(set(cluster_labels)):\n",
        "            index = frame[frame['label'] == m].index\n",
        "            subset = rep[index]\n",
        "            centroid = np.mean(rep[index], axis=0)\n",
        "            centroids[m] = centroid\n",
        "\n",
        "        centroid_keywords = {}\n",
        "        for key in centroids.keys():\n",
        "            centroid = centroids[key]\n",
        "            similarity = cosine_similarity([centroid], rep_rep)\n",
        "            centroid_keyword_index = similarity[0].argsort()[-3:][::-1]\n",
        "            centroid_keywords[key] = [total[i] for i in centroid_keyword_index]\n",
        "\n",
        "        return centroid_keywords\n",
        "\n",
        "    def pipeline(self, dimension=5, clustering_method='hdbscan', min_cluster_size=2, n_clusters=5):\n",
        "        embedding = self.encoding()\n",
        "        reduced_embedding = self.DR(embedding, dimension=dimension)\n",
        "\n",
        "        globals()['reduced_embedding'] = reduced_embedding\n",
        "        if clustering_method == 'agglomerative':\n",
        "            cluster_labels, _ = self.agglomerative_clustering(reduced_embedding, n_clusters=n_clusters)\n",
        "        if clustering_method == 'hdbscan':\n",
        "            cluster_labels, _ = self.clustering(reduced_embedding, min_cluster_size=min_cluster_size)\n",
        "\n",
        "        candidate_vocab = self.build_candidates(self.corpus, self.encoder)\n",
        "        centroid_keywords = self.centroid(embedding, candidate_vocab, cluster_labels)\n",
        "        return cluster_labels, centroid_keywords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
